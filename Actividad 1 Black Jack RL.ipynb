{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cd156c",
   "metadata": {},
   "source": [
    "# Actividad 1\n",
    "# Black Jack Utilizando RL DQN\n",
    "## Dario Castro 719910\n",
    "## Ingenieria Financiera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "745b40f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{18: 14649, 'Bust': 30504, 19: 13964, 21: 12193, 20: 13293, 17: 15397}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "def draw():\n",
    "    cards = [2,3,4,5,6,7,8,9,10,10,10,10,11]\n",
    "    return random.choice(cards)\n",
    "\n",
    "def deal():\n",
    "    hand = [draw(), draw()]\n",
    "    return sum(hand), hand\n",
    "def Naive_blackjack():\n",
    "    ##starting hand\n",
    "    aces = 0\n",
    "    total = random.randint(2,11)\n",
    "    if total == 11:\n",
    "        aces = 1\n",
    "    drawn = draw()\n",
    "    if drawn == 11:\n",
    "        aces += 1\n",
    "    while total < 17:\n",
    "        total += drawn\n",
    "        if total > 21 and aces > 0:\n",
    "            total -= 10\n",
    "            aces -= 1\n",
    "        drawn = draw()\n",
    "        if drawn == 11:\n",
    "            aces += 1\n",
    "    if total > 21:\n",
    "        return 'Bust'\n",
    "    return total \n",
    "\n",
    "\n",
    "def simulate_blackjack(n):\n",
    "    results = {}\n",
    "    for i in range(n):\n",
    "        play = (Naive_blackjack())\n",
    "        if play in results:\n",
    "            results[play] += 1\n",
    "        else:\n",
    "            results[play] = 1\n",
    "    return (results)\n",
    "\n",
    "simulate_blackjack(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2695887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from typing import Optional, Tuple, Union\n",
    "from gymnasium import logger, spaces\n",
    "\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "324fc564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackJackEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(BlackJackEnv, self).__init__()\n",
    "        self.action_space = gym.spaces.Discrete(2)  # Hit or Stand\n",
    "        self.observation_space = gym.spaces.Box(low=np.array([0, 0]), high=np.array([21, 4]), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def rewardfnc(self, score):\n",
    "        return {21: 1.0, 20: 0.878, 19: 0.746, 18: 0.606, 17: 0.458}.get(score, 0.303)\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:  # Stand\n",
    "            return np.array([self.score, self.aces], dtype=np.float32), self.rewardfnc(self.score), True, {}\n",
    "        \n",
    "        card = draw()\n",
    "        self.score += card\n",
    "        if card == 11:\n",
    "            self.aces += 1\n",
    "\n",
    "        if self.score > 21 and self.aces > 0:\n",
    "            self.score -= 10\n",
    "            self.aces -= 1\n",
    "\n",
    "        done = self.score > 21\n",
    "        reward = -1 if done else 0\n",
    "        return np.array([self.score, self.aces], dtype=np.float32), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.score, self.hand = deal()\n",
    "        self.aces = sum(1 for card in self.hand if card == 11)\n",
    "        return np.array([self.score, self.aces], dtype=np.float32)\n",
    "\n",
    "env = BlackJackEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f5c84d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67cac1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: 0.30300000309944153\n",
      "Episode 100, Total Reward: -1.0\n",
      "Episode 200, Total Reward: 0.30300000309944153\n",
      "Episode 300, Total Reward: 0.30300000309944153\n",
      "Episode 400, Total Reward: 0.30300000309944153\n",
      "Episode 500, Total Reward: 0.7459999918937683\n",
      "Episode 600, Total Reward: 0.4580000042915344\n",
      "Episode 700, Total Reward: -1.0\n",
      "Episode 800, Total Reward: 0.7459999918937683\n",
      "Episode 900, Total Reward: 0.30300000309944153\n",
      "Episode 1000, Total Reward: 0.30300000309944153\n",
      "Episode 1100, Total Reward: 0.30300000309944153\n",
      "Episode 1200, Total Reward: 0.30300000309944153\n",
      "Episode 1300, Total Reward: 0.878000020980835\n",
      "Episode 1400, Total Reward: 0.30300000309944153\n",
      "Episode 1500, Total Reward: 0.878000020980835\n",
      "Episode 1600, Total Reward: -1.0\n",
      "Episode 1700, Total Reward: 0.30300000309944153\n",
      "Episode 1800, Total Reward: 0.4580000042915344\n",
      "Episode 1900, Total Reward: 0.7459999918937683\n",
      "Episode 2000, Total Reward: 0.30300000309944153\n",
      "Episode 2100, Total Reward: 0.30300000309944153\n",
      "Episode 2200, Total Reward: 0.30300000309944153\n",
      "Episode 2300, Total Reward: 0.30300000309944153\n",
      "Episode 2400, Total Reward: 0.30300000309944153\n",
      "Episode 2500, Total Reward: 0.30300000309944153\n",
      "Episode 2600, Total Reward: 0.30300000309944153\n",
      "Episode 2700, Total Reward: -1.0\n",
      "Episode 2800, Total Reward: 0.878000020980835\n",
      "Episode 2900, Total Reward: 1.0\n",
      "Episode 3000, Total Reward: 0.30300000309944153\n",
      "Episode 3100, Total Reward: 0.30300000309944153\n",
      "Episode 3200, Total Reward: 0.30300000309944153\n",
      "Episode 3300, Total Reward: 0.878000020980835\n",
      "Episode 3400, Total Reward: 0.30300000309944153\n",
      "Episode 3500, Total Reward: 0.30300000309944153\n",
      "Episode 3600, Total Reward: 0.30300000309944153\n",
      "Episode 3700, Total Reward: 0.30300000309944153\n",
      "Episode 3800, Total Reward: 0.30300000309944153\n",
      "Episode 3900, Total Reward: 0.30300000309944153\n",
      "Episode 4000, Total Reward: 0.4580000042915344\n",
      "Episode 4100, Total Reward: 0.6060000061988831\n",
      "Episode 4200, Total Reward: 0.30300000309944153\n",
      "Episode 4300, Total Reward: 1.0\n",
      "Episode 4400, Total Reward: 0.30300000309944153\n",
      "Episode 4500, Total Reward: -1.0\n",
      "Episode 4600, Total Reward: 0.30300000309944153\n",
      "Episode 4700, Total Reward: 0.30300000309944153\n",
      "Episode 4800, Total Reward: 0.878000020980835\n",
      "Episode 4900, Total Reward: 0.30300000309944153\n",
      "Episode 5000, Total Reward: 0.30300000309944153\n",
      "Episode 5100, Total Reward: 0.4580000042915344\n",
      "Episode 5200, Total Reward: 0.878000020980835\n",
      "Episode 5300, Total Reward: 0.4580000042915344\n",
      "Episode 5400, Total Reward: 0.878000020980835\n",
      "Episode 5500, Total Reward: 1.0\n",
      "Episode 5600, Total Reward: 0.30300000309944153\n",
      "Episode 5700, Total Reward: 0.30300000309944153\n",
      "Episode 5800, Total Reward: 1.0\n",
      "Episode 5900, Total Reward: 0.30300000309944153\n",
      "Episode 6000, Total Reward: 0.30300000309944153\n",
      "Episode 6100, Total Reward: 0.4580000042915344\n",
      "Episode 6200, Total Reward: 0.7459999918937683\n",
      "Episode 6300, Total Reward: 0.30300000309944153\n",
      "Episode 6400, Total Reward: 0.878000020980835\n",
      "Episode 6500, Total Reward: 0.30300000309944153\n",
      "Episode 6600, Total Reward: 0.30300000309944153\n",
      "Episode 6700, Total Reward: 0.30300000309944153\n",
      "Episode 6800, Total Reward: 0.878000020980835\n",
      "Episode 6900, Total Reward: 0.30300000309944153\n",
      "Episode 7000, Total Reward: 0.6060000061988831\n",
      "Episode 7100, Total Reward: 0.30300000309944153\n",
      "Episode 7200, Total Reward: 0.30300000309944153\n",
      "Episode 7300, Total Reward: 0.30300000309944153\n",
      "Episode 7400, Total Reward: 0.30300000309944153\n",
      "Episode 7500, Total Reward: -1.0\n",
      "Episode 7600, Total Reward: 0.30300000309944153\n",
      "Episode 7700, Total Reward: 0.30300000309944153\n",
      "Episode 7800, Total Reward: 0.6060000061988831\n",
      "Episode 7900, Total Reward: 0.7459999918937683\n",
      "Episode 8000, Total Reward: 0.30300000309944153\n",
      "Episode 8100, Total Reward: 0.4580000042915344\n",
      "Episode 8200, Total Reward: 0.6060000061988831\n",
      "Episode 8300, Total Reward: 0.30300000309944153\n",
      "Episode 8400, Total Reward: 0.30300000309944153\n",
      "Episode 8500, Total Reward: 0.6060000061988831\n",
      "Episode 8600, Total Reward: 0.30300000309944153\n",
      "Episode 8700, Total Reward: 0.30300000309944153\n",
      "Episode 8800, Total Reward: 0.30300000309944153\n",
      "Episode 8900, Total Reward: 0.30300000309944153\n",
      "Episode 9000, Total Reward: 0.30300000309944153\n",
      "Episode 9100, Total Reward: 0.878000020980835\n",
      "Episode 9200, Total Reward: 0.30300000309944153\n",
      "Episode 9300, Total Reward: 0.7459999918937683\n",
      "Episode 9400, Total Reward: 0.30300000309944153\n",
      "Episode 9500, Total Reward: 0.30300000309944153\n",
      "Episode 9600, Total Reward: 0.30300000309944153\n",
      "Episode 9700, Total Reward: 0.30300000309944153\n",
      "Episode 9800, Total Reward: 0.30300000309944153\n",
      "Episode 9900, Total Reward: 0.30300000309944153\n"
     ]
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_observations, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "    \n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 5000\n",
    "LR = 1e-4\n",
    "TAU = 0.005\n",
    "\n",
    "# DQN Setup\n",
    "n_actions = env.action_space.n\n",
    "n_observations = 2\n",
    "policy_net = DQN(n_observations, n_actions)\n",
    "target_net = DQN(n_observations, n_actions)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "memory = ReplayMemory(10000)\n",
    "steps_done = 0\n",
    "\n",
    "# Epsilon-Greedy Action Selection\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * np.exp(-steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if random.random() > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).argmax(dim=1).view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], dtype=torch.long)\n",
    "    \n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "num_episodes = 10000\n",
    "total_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    episode_reward = 0\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        reward = torch.tensor([reward], dtype=torch.float32)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0) if not done else None\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model()\n",
    "        \n",
    "        episode_reward += reward.item()\n",
    "        if done:\n",
    "            total_rewards.append(episode_reward)\n",
    "            break\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeed0943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de episodios: 10000\n",
      "Episodios ganados: 8984\n",
      "Porcentaje de victorias: 89.84%\n"
     ]
    }
   ],
   "source": [
    "# Convertir total_rewards a numpy array\n",
    "total_rewards = np.array(total_rewards, dtype=np.float32)\n",
    "\n",
    "# Contar victorias (recompensa > 0)\n",
    "wins = np.sum(total_rewards > 0)\n",
    "total_episodes = len(total_rewards)\n",
    "\n",
    "# Calcular porcentaje de victorias\n",
    "win_percentage = (wins / total_episodes) * 100\n",
    "\n",
    "print(f\"Total de episodios: {total_episodes}\")\n",
    "print(f\"Episodios ganados: {wins}\")\n",
    "print(f\"Porcentaje de victorias: {win_percentage:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12349384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
